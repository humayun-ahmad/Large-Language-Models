{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Continuous Bag of Words(CBOW)",
   "id": "ec5a7c061febf77a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocess the Text",
   "id": "fb881ba40e4f8bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:14:39.421479Z",
     "start_time": "2025-01-15T11:14:17.036125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter"
   ],
   "id": "74cb8b8f1277d5af",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:18:45.181614Z",
     "start_time": "2025-01-15T11:18:44.969171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the Sherlock Holmes text\n",
    "with open('sherlock_holmes.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "# Clean and tokenize the text\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "tokens = preprocess(text)\n",
    "\n",
    "# Create a vocabulary and word-to-index mapping\n",
    "vocab = Counter(tokens)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "# Map words to indices and vice versa\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "# print(word_to_idx)\n",
    "idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "# print(idx_to_word)\n",
    "# print(\"Vocabulary:\", vocab)\n",
    "# print(\"Word to Index:\", word_to_idx)\n"
   ],
   "id": "25636a0806c68523",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8699\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prepare Training Data",
   "id": "282d048a7f6b852"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:16:35.674245Z",
     "start_time": "2025-01-15T11:16:34.955917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define context window size\n",
    "window_size = 2\n",
    "\n",
    "# Generate context-target pair\n",
    "data = []\n",
    "for i in range(window_size, len(tokens) - window_size):\n",
    "    context = [tokens[i - j] for j in range(1, window_size + 1)]\n",
    "    context += [tokens[i + j] for j in range(1, window_size + 1)]\n",
    "    target = tokens[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "# print(\"Context-Target Pairs:\", data)"
   ],
   "id": "708c6c39f150ce31",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prepare Training Data",
   "id": "78f7425c215ee246"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:20:24.038964Z",
     "start_time": "2025-01-15T11:20:23.424165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the context window size\n",
    "window_size = 2\n",
    "\n",
    "def generate_context_target_pairs(words, window_size):\n",
    "    data = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        target = words[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "data = generate_context_target_pairs(tokens, window_size)\n",
    "\n",
    "# Convert words to their indices\n",
    "def words_to_indices(data, word2idx):\n",
    "    context_indices = []\n",
    "    target_indices = []\n",
    "    for context, target in data:\n",
    "        context_indices.append([word2idx[word] for word in context])\n",
    "        target_indices.append(word2idx[target])\n",
    "    return np.array(context_indices), np.array(target_indices)\n",
    "\n",
    "context_indices, target_indices = words_to_indices(data, word2idx)\n"
   ],
   "id": "e84b438288b4d6c5",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Build the CBOW Model",
   "id": "7545d2b62c6c01f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:27:59.402746Z",
     "start_time": "2025-01-15T11:27:59.376543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 100  # You can change the embedding size\n",
    "\n",
    "# Define the model\n",
    "class CBOW(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embeddings(inputs)\n",
    "        x = tf.reduce_mean(x, axis=1)  # Average the context embeddings\n",
    "        return self.dense(x)\n",
    "\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ],
   "id": "551bb76515887e05",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the Model",
   "id": "85f7874101347d68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T12:03:17.500129Z",
     "start_time": "2025-01-15T11:28:01.795618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "model.fit(context_indices, target_indices, epochs=10, batch_size=128)\n"
   ],
   "id": "ad8f3616bf36cdb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m210s\u001B[0m 249ms/step - accuracy: 0.0565 - loss: 7.6789\n",
      "Epoch 2/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m199s\u001B[0m 237ms/step - accuracy: 0.0816 - loss: 6.2904\n",
      "Epoch 3/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m171s\u001B[0m 203ms/step - accuracy: 0.0978 - loss: 6.0069\n",
      "Epoch 4/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m251s\u001B[0m 299ms/step - accuracy: 0.1148 - loss: 5.7807\n",
      "Epoch 5/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m240s\u001B[0m 273ms/step - accuracy: 0.1350 - loss: 5.5385\n",
      "Epoch 6/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m185s\u001B[0m 221ms/step - accuracy: 0.1509 - loss: 5.3590\n",
      "Epoch 7/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m179s\u001B[0m 213ms/step - accuracy: 0.1682 - loss: 5.1639\n",
      "Epoch 8/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m192s\u001B[0m 229ms/step - accuracy: 0.1817 - loss: 5.0294\n",
      "Epoch 9/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m302s\u001B[0m 348ms/step - accuracy: 0.1957 - loss: 4.8537\n",
      "Epoch 10/10\n",
      "\u001B[1m840/840\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m185s\u001B[0m 220ms/step - accuracy: 0.2099 - loss: 4.7201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x232bed3d650>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the Model with New Inputs",
   "id": "56910e507101c20a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T12:09:04.045575Z",
     "start_time": "2025-01-15T12:09:03.805463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_target(context_words, model, word2idx, idx2word):\n",
    "    context_indices = np.array([[word2idx[word] for word in context_words]])\n",
    "    prediction = model.predict(context_indices)\n",
    "    predicted_word_idx = np.argmax(prediction)\n",
    "    return idx2word[predicted_word_idx]\n",
    "\n",
    "# Test example: predict the word in the middle of the context\n",
    "test_context = ['brow', 'dog', 'quick', 'over'] # ['quick', 'brown', 'fox', 'jumps']  # Example context\n",
    "predicted_word = predict_target(test_context, model, word2idx, idx2word)\n",
    "print(f\"Predicted word: {predicted_word}\")\n"
   ],
   "id": "152338bc17487437",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 131ms/step\n",
      "Predicted word: his\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
